{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "from timeit import default_timer\n",
    "import json\n",
    "#this method gets all the records from the API\n",
    "#it takes the location of the json file and the access code of the api as parameters\n",
    "def getRecords(jsonFile, code, max_records):\n",
    "\n",
    "    client = Socrata(\"data.cityofnewyork.us\", \"mInZoLergBidZkJY82xc7FYke\")\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "# client = Socrata(data.cityofnewyork.us,\n",
    "#                  MyAppToken,\n",
    "#                  userame=\"user@example.com\",\n",
    "#                  password=\"AFakePassword\")\n",
    "\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "    start = default_timer()\n",
    "\n",
    "# do stuff\n",
    "#131165043\n",
    "#16385532\n",
    "    results = client.get(code,limit=max_records)\n",
    "# Convert to pandas DataFrame\n",
    "    results_dfYellow = pd.DataFrame.from_records(results)\n",
    "\n",
    "    with open(jsonFile, 'w') as outfile:\n",
    "        json.dump(results, outfile)\n",
    "\n",
    "    duration = default_timer() - start\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "#this method converts the json file to a csv file\n",
    "def convertJsonToCsv(jsonfile, csvfile, fieldnames): \n",
    "    \n",
    "\n",
    "    start = default_timer()\n",
    "\n",
    "    infile = open(jsonfile, \"r\")\n",
    "\n",
    "    outfile = open(csvfile, \"w\")\n",
    "\n",
    "    #uses the fields given as a parameter \n",
    "    writer = csv.DictWriter(outfile, fieldnames = fieldnames, delimiter = ';', lineterminator='\\n')\n",
    "    \n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "\n",
    "\n",
    "#writer.writerow([\"dropoff_latitude\", \"dropoff_longitude\", \"extra\", \"fare_amount\",\n",
    "#\n",
    "#                 \"improvement_surcharge\", \"mta_tax\", \"passenger_count\", \"payment_type\",\n",
    "#\n",
    "#                 \"pickup_latitude\", \"pickup_longitude\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "#\n",
    "#                 \"tip_amount\", \"tolls_amount\", \"total_amount\", \"tpep_dropoff_datetime\",\n",
    "#\n",
    "#                 \"tpep_pickup_datetime\", \"trip_distance\", \"vendorid\"])\n",
    "\n",
    "    for row in json.loads(infile.read()):\n",
    "               \n",
    "        row_json = {}\n",
    "        \n",
    "        #for every row in the json file it adds keys and values to the dictionary\n",
    "        for attribute, value in row.items():\n",
    "            if attirbute != 'dolocationid' and attribute != 'pulocationid':\n",
    "                row_json[attribute] = value\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        writer.writerow(row_json)\n",
    "\n",
    "#        writer.writerow({\"dropoff_latitude\": row[\"dropoff_latitude\"], \"dropoff_longitude\": row[\"dropoff_longitude\"],\n",
    "                     \n",
    "#                         \"extra\": row[\"extra\"], \"fare_amount\": row[\"fare_amount\"],\n",
    "\n",
    "#                         \"improvement_surcharge\": row[\"improvement_surcharge\"], \"mta_tax\": row[\"mta_tax\"],\n",
    "                     \n",
    "#                         \"passenger_count\": row[\"passenger_count\"], \"payment_type\": row[\"payment_type\"],\n",
    "\n",
    "#                         \"pickup_latitude\": row[\"pickup_latitude\"], \"pickup_longitude\": row[\"pickup_longitude\"],\n",
    "                     \n",
    "#                         \"ratecodeid\": row[\"ratecodeid\"], \"store_and_fwd_flag\": row[\"store_and_fwd_flag\"],\n",
    "\n",
    "#                         \"tip_amount\": row[\"tip_amount\"], \"tolls_amount\": row[\"tolls_amount\"],\n",
    "                     \n",
    "#                         \"total_amount\": row[\"total_amount\"], \"lpep_dropoff_datetime\": row[\"lpep_dropoff_datetime\"],\n",
    "\n",
    "#                         \"lpep_pickup_datetime\": row[\"lpep_pickup_datetime\"], \"trip_distance\": row[\"trip_distance\"],\n",
    "                     \n",
    "#                         \"vendorid\": row[\"vendorid\"]})\n",
    "\n",
    "    \n",
    "\n",
    "    infile.close()\n",
    "    outfile.close()\n",
    "    \n",
    "    duration = default_timer() - start\n",
    "\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-66cabee1067b>:6 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-66cabee1067b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\steven\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mc:\\users\\steven\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    297\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 299\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-66cabee1067b>:6 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#add fields to the rdd\n",
    "def addFields(line, separator): \n",
    "    fields = []\n",
    "    \n",
    "    values = line.split(separator)\n",
    "    \n",
    "    #dropoff_latitude\n",
    "    fields.append(float(values[0]))\n",
    "    \n",
    "    #dropoff_longitude\n",
    "    fields.append(float(values[1]))\n",
    "    \n",
    "    #extra\n",
    "    fields.append(float(values[2]))\n",
    "    \n",
    "    #fare_amount\n",
    "    fields.append(float(values[3]))\n",
    "    \n",
    "    #improvement_surcharge\n",
    "    fields.append(float(values[4]))\n",
    "    \n",
    "    #mta_tax\n",
    "    fields.append(float(values[5]))\n",
    "    \n",
    "    #passenger_count\n",
    "    fields.append(int(values[6]))\n",
    "    \n",
    "    #payment_type\n",
    "    fields.append(int(values[7]))\n",
    "    \n",
    "    #pickup_latitude\n",
    "    fields.append(float(values[8]))\n",
    "    \n",
    "    #pickup_longitude\n",
    "    fields.append(float(values[9]))\n",
    "    \n",
    "    #ratecodeid\n",
    "    fields.append(int(values[10]))\n",
    "    \n",
    "    #store_and_fwd_flag\n",
    "    fields.append(str(values[11]))\n",
    "    \n",
    "    #tip_amount\n",
    "    fields.append(float(values[12]))\n",
    "    \n",
    "    #tolls_amount\n",
    "    fields.append(float(values[13]))\n",
    "    \n",
    "    #total_amount\n",
    "    fields.append(float(values[14]))\n",
    "    \n",
    "    #lpep_dropoff_datetime\n",
    "    fields.append(datetime.strptime(values[15][:-4], '%Y-%m-%dT%H:%M:%S'))\n",
    "    \n",
    "    #lpep_pickup_datetime\n",
    "    fields.append(datetime.strptime(values[16][:-4], '%Y-%m-%dT%H:%M:%S'))\n",
    "    \n",
    "    #trip_distance\n",
    "    fields.append(float(values[17]))\n",
    "    \n",
    "    #vendorid\n",
    "    fields.append(int(values[18]))\n",
    "    \n",
    "    return fields\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "\n",
    "\n",
    "infile_Yellow = \"taxis_Yellow.json\"\n",
    "outfile_Yellow = \"taxis_Yellow.csv\"\n",
    "accesscode_Yellow = \"uacg-pexx\"\n",
    "max_records_Yellow = 131165043\n",
    "fieldnames_Yellow = [\"dropoff_latitude\", \"dropoff_longitude\", \"extra\", \"fare_amount\",\n",
    "                    \"improvement_surcharge\", \"mta_tax\", \"passenger_count\", \"payment_type\",\n",
    "                     \"pickup_latitude\", \"pickup_longitude\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "                     \"tip_amount\", \"tolls_amount\", \"total_amount\", \"tpep_dropoff_datetime\",\n",
    "                     \"tpep_pickup_datetime\", \"trip_distance\", \"vendorid\"]\n",
    "\n",
    "infile_Green = \"taxis_Green.json\"\n",
    "outfile_Green = \"taxis_Green.csv\"\n",
    "accesscode_Green = \"pqfs-mqru\"\n",
    "max_records_Green = 16385532\n",
    "fieldnames_Green = [\"dropoff_latitude\", \"dropoff_longitude\", \"extra\", \"fare_amount\",\n",
    "                    \"improvement_surcharge\", \"mta_tax\", \"passenger_count\", \"payment_type\",\n",
    "                     \"pickup_latitude\", \"pickup_longitude\", \"ratecodeid\", \"store_and_fwd_flag\",\n",
    "                     \"tip_amount\", \"tolls_amount\", \"total_amount\", \"lpep_dropoff_datetime\",\n",
    "                     \"lpep_pickup_datetime\", \"trip_distance\", \"trip_type\", \"vendorid\"]\n",
    "\n",
    "#\"_id\", \"dropoff_latitude\", \"dropoff_longitude\", \"extra\", \"fare_amount\", \"improvement_surcharge\", \"lpep_dropoff_datetime\"\n",
    "#\"lpep_pickup_datetime\", \"mta_tax\", \"passenger_count\", \"payment_type\", \"pickup_latitude\", \"pickup_longitude\", \"ratecodeid\"\n",
    "#\"store_and_fwd_flag\", tip_amount\", \"tolls_amount\", \"total_amount\", \"trip_distance\", \"trip_type\", \"vendorid\"\n",
    "\n",
    "def convertToDf(csvFile, schema):\n",
    "    \n",
    "    separator = ';'\n",
    "    \n",
    "    taxis_data = sc.textFile(csvFile)\n",
    "    \n",
    "    header = taxis_data.first()\n",
    "    taxis_data = taxis_data.filter(lambda row: row != header)\n",
    "    \n",
    "    taxis_rdd = (taxis_data\n",
    "                    .map(lambda l: addFields(l, separator)))\n",
    "                    \n",
    "    \n",
    "    #header = taxis_rdd.first\n",
    "    #taxis_rdd = taxis_rdd.filter(lambda l: l != header)\n",
    "    \n",
    "    taxis_df = sqlContext.createDataFrame(taxis_rdd, schema)\n",
    "    \n",
    "    return taxis_df\n",
    "\n",
    "\n",
    "def createSchema(yellow):\n",
    "\n",
    "    fields = []\n",
    "\n",
    "    fields.append(StructField('dropoff_latitude', FloatType(), True))\n",
    "    fields.append(StructField('dropoff_longitude', FloatType(), True))\n",
    "    fields.append(StructField('extra', FloatType(), True))\n",
    "    fields.append(StructField('fare_amount', FloatType(), True))\n",
    "    fields.append(StructField('improvement_surcharge', FloatType(), True))\n",
    "    fields.append(StructField('mta_tax', FloatType(), True))\n",
    "    fields.append(StructField('passenger_count', IntegerType(), True))\n",
    "    fields.append(StructField('payment_type', IntegerType(), True))\n",
    "    fields.append(StructField('pickup_latitude', FloatType(), True))\n",
    "    fields.append(StructField('pickup_longitude', FloatType(), True))\n",
    "    fields.append(StructField('ratecodeid', IntegerType(), True))\n",
    "    fields.append(StructField('store_and_fwd_flag', StringType(), True))\n",
    "    fields.append(StructField('tip_amount', FloatType(), True))\n",
    "    fields.append(StructField('tolls_amount', FloatType(), True))\n",
    "    fields.append(StructField('total_amount', FloatType(), True))\n",
    "    \n",
    "    if yellow == True:      \n",
    "        fields.append(StructField('tpep_dropoff_datetime', DateType(), True))\n",
    "        fields.append(StructField('tpep_pickup_datetime', DateType(), True))\n",
    "        \n",
    "    else: \n",
    "        fields.append(StructField('lpep_dropoff_datetime', DateType(), True))\n",
    "        fields.append(StructField('lpep_pickup_datetime', DateType(), True))\n",
    "        \n",
    "    fields.append(StructField('trip_distance', FloatType(), True))\n",
    "    fields.append(StructField('vendorid', IntegerType(), True))\n",
    "    schema = StructType(fields)\n",
    "    return schema\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "sqlContext=SQLContext(sc)\n",
    "\n",
    "timeDict_Yellow = {}\n",
    "timeDict_Green = {}\n",
    "\n",
    "timeDict_Yellow[\"Get Records\"] = getRecords(infile_Yellow, accesscode_Yellow, max_records_Yellow)\n",
    "timeDict_Green[\"Get Records\"] = getRecords(infile_Green, accesscode_Green, max_records_Green)\n",
    "\n",
    "timeDict_Yellow[\"Convert to csv\"] = convertJsonToCsv(infile_Yellow, outfile_Yellow, fieldnames_Yellow)\n",
    "timeDict_Green[\"Convet to csv\"] = convertJsonToCsv(infile_Green, outfile_Green, fieldnames_Green)\n",
    "\n",
    "start = default_timer()\n",
    "convertToDf(outfile_Yellow, createSchema(True)).registerTempTable('Yellow_table')\n",
    "duration = default_timer() - start\n",
    "timeDict_Yellow[\"Convert to df\"] = duration\n",
    "\n",
    "start = default_timer()\n",
    "convertToDf(outfile_Green, createSchema(False)).registerTempTable('Green_table')\n",
    "duration = default_timer() - start\n",
    "timeDict_Green[\"Convert to df\"] = duration\n",
    "\n",
    "print(\"Yellow:\" + str(timeDict_Yellow))\n",
    "print(\"Green:\" + str(timeDict_Green))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = default_timer()\n",
    "\n",
    "sql_Yellow = \"SELECT payment_type, COUNT(*) as count FROM Yellow_table GROUP BY payment_type ORDER BY payment_type ASC\"\n",
    "payment_yellow = sqlContext.sql(sql_Yellow)\n",
    "\n",
    "payment_yellow = payment_yellow.rdd.map(lambda row: int(row[1]))\n",
    "\n",
    "payment_yellow_arr = []\n",
    "for index in payment_yellow.collect():\n",
    "    payment_yellow_arr.append(index)\n",
    "    \n",
    "duration = default_timer() - start\n",
    "timeDict_Yellow[\"payment type query\"] = duration\n",
    "\n",
    "start = default_timer()\n",
    "\n",
    "sql_Green = \"SELECT payment_type, COUNT(*) as count FROM Green_table GROUP BY payment_type ORDER BY payment_type ASC\"\n",
    "payment_green = sqlContext.sql(sql_Green)\n",
    "\n",
    "payment_green = payment_green.rdd.map(lambda row: int(row[1]))\n",
    "\n",
    "payment_green_arr = []\n",
    "for index in payment_green.collect():\n",
    "    payment_green_arr.append(index)\n",
    "    \n",
    "duration = default_timer() - start\n",
    "timeDict_Green[\"payment type query\"] = duration    \n",
    "    \n",
    "print(str(payment_yellow_arr))\n",
    "print(str(payment_green_arr))\n",
    "\n",
    "print(str(timeDict_Yellow))\n",
    "print(str(timeDict_Green))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# data to plot\n",
    "n_groups = 2\n",
    "payGreen = []\n",
    "payGreen.extend((payment_green_arr[0],payment_green_arr[1]))\n",
    "payYellow = []\n",
    "payYellow.extend((payment_yellow_arr[0],payment_yellow_arr[1]))\n",
    "\n",
    "print(payGreen)\n",
    "# create plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "opacity = 0.8\n",
    " \n",
    "rects1 = plt.bar(index, payGreen, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='green',\n",
    "                 label='Green')\n",
    " \n",
    "rects2 = plt.bar(index + bar_width, payYellow, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='yellow',\n",
    "                 label='Yellow')\n",
    "#,'No Charge','Dispute','Unknown','Voided trip'\n",
    "plt.ylabel('Aantal betaling')\n",
    "plt.title('Vergelijking betaling Yellow & Green Taxi company NYC ')\n",
    "plt.xlabel('Payment_type')\n",
    "x_axis=['Creditcard','Cash']\n",
    "plt.xticks(index+bar_width,x_axis)\n",
    "plt.legend()\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
